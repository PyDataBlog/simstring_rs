import json
import platform
import sys
from pathlib import Path

import polars as pl
import psutil
from tabulate import tabulate


def get_system_specs():
    specs = {
        "System": platform.system(),
        "Release": platform.release(),
        "Version": platform.version(),
        "Machine": platform.machine(),
        "Processor": platform.processor(),
        "CPU Cores": psutil.cpu_count(logical=False),
        "Logical CPUs": psutil.cpu_count(logical=True),
        "Total Memory": f"{psutil.virtual_memory().total / (1024**3):.2f} GB",
    }
    return specs


def compare_benchmarks():
    try:
        print("--- Starting benchmark comparison ---")
        benches_dir = Path(__file__).parent
        results_path = benches_dir / "results.json"
        output_path = benches_dir.parent / "BENCHMARKS.md"

        print(f"Reading results from: {results_path}")
        if not results_path.exists():
            print(f"Error: results.json not found at {results_path}!", file=sys.stderr)
            sys.exit(1)

        with open(results_path) as f:
            data = json.load(f)

        if not data:
            print("Error: results.json is empty!", file=sys.stderr)
            sys.exit(1)

        print("Successfully loaded results.json. Normalizing data with Polars.")
        df = pl.from_dicts(data)
        # Unnest parameter and stats dictionaries
        df = df.unnest("parameters").unnest("stats")

        print("Sorting data.")
        df = df.sort(["benchmark", "language", "backend"])

        print(f"Writing markdown to: {output_path}")
        with open(output_path, "w") as f:
            f.write(
                "This file is automatically generated by the CI. Do not edit manually.\n\n"
            )

            f.write("### Hardware Specifications\n")
            specs = get_system_specs()
            for key, value in specs.items():
                f.write(f"- **{key}**: {value}\n")
            f.write("\n")

            for benchmark_name, group in df.group_by("benchmark"):
                f.write(f"### {str(benchmark_name).capitalize()} Benchmark\n")

                # Select columns to display, dropping any that are all null
                display_cols = [
                    col for col in group.columns if group[col].is_not_null().any()
                ]
                group_display = group.select(display_cols)

                # Create markdown table
                markdown_output = tabulate(
                    group_display, headers="keys", tablefmt="pipe", showindex=False
                )

                if markdown_output:
                    f.write(markdown_output)

        print("--- Finished benchmark comparison successfully ---")

    except Exception as e:
        print(f"An unexpected error occurred: {e}", file=sys.stderr)
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    compare_benchmarks()
