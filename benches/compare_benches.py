import json
import sys
from pathlib import Path

import pandas as pd
from pydantic_ai import Agent
from tenacity import retry, stop_after_attempt, wait_exponential

agent = Agent(
    "google-gla:gemini-2.5-pro",
    system_prompt=(
        "Directly generate a markdown summary of the provided benchmark results. "
        "Do not include any conversational filler or introductory sentences. "
        "Your response should begin immediately with the summary content, "
        "structured with markdown sub-headings (e.g., '#### Summary'). "
        "The summary should analyze insert and search performance, "
        "compare the different implementations, and include speedup metrics."
    ),
)


@retry(wait=wait_exponential(multiplier=1, min=4, max=10), stop=stop_after_attempt(3))
def generate_summary(agent: Agent, benchmark_markdown_output: str) -> str:
    results = agent.run_sync(benchmark_markdown_output)
    return results.output


def compare_benchmarks():
    try:
        print("--- Starting benchmark comparison ---")
        benches_dir = Path(__file__).parent
        results_path = benches_dir / "results.json"
        output_path = benches_dir.parent / "BENCHMARKS.md"

        print(f"Reading results from: {results_path}")
        if not results_path.exists():
            print(f"Error: results.json not found at {results_path}!", file=sys.stderr)
            sys.exit(1)

        with open(results_path) as f:
            data = json.load(f)

        if not data:
            print("Error: results.json is empty!", file=sys.stderr)
            sys.exit(1)

        print("Successfully loaded results.json. Normalizing data with pandas.")
        df = pd.json_normalize(data)

        print("Sorting data.")
        df = df.sort_values(["benchmark", "language", "backend"])

        print(f"Writing markdown to: {output_path}")
        with open(output_path, "w") as f:
            f.write(
                "This file is automatically generated by the CI. Do not edit manually.\n\n"
            )

            for benchmark, group in df.groupby("benchmark"):
                f.write(f"### {str(benchmark).capitalize()} Benchmark\n")

                param_cols = [
                    col.replace("parameters.", "")
                    for col in df.columns
                    if col.startswith("parameters.")
                ]
                display_cols = (
                    ["language", "backend"]
                    + [f"parameters.{p}" for p in param_cols]
                    + ["stats.mean", "stats.stddev", "stats.iterations"]
                )

                display_cols = [col for col in display_cols if col in group.columns]

                group = group[display_cols].dropna(axis=1, how="all")

                group.columns = [
                    col.replace("parameters.", "").replace("stats.", "")
                    for col in group.columns
                ]

                markdown_output = group.to_markdown(index=False)
                if markdown_output:
                    f.write(markdown_output)
                    # Generate a comparision summary section via LLM which includes the speedup gains and/os losses
                    f.write("\n\n#### AI Summary\n")
                    summary = generate_summary(agent, markdown_output)
                    f.write(summary)
                f.write("\n\n")

        print("--- Finished benchmark comparison successfully ---")

    except Exception as e:
        print(f"An unexpected error occurred: {e}", file=sys.stderr)
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    compare_benchmarks()
