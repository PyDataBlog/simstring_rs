import json
import platform
import sys
from pathlib import Path

import polars as pl
import psutil
from tabulate import tabulate


def get_system_specs():
    specs = {
        "System": platform.system(),
        "Release": platform.release(),
        "Version": platform.version(),
        "Machine": platform.machine(),
        "Processor": platform.processor(),
        "CPU Cores": psutil.cpu_count(logical=False),
        "Logical CPUs": psutil.cpu_count(logical=True),
        "Total Memory": f"{psutil.virtual_memory().total / (1024**3):.2f} GB",
    }
    return specs


def generate_manual_summary(df: pl.DataFrame) -> str:
    rust_native_backend = "simstring-rust (native)"

    # Identify parameter columns dynamically
    base_cols = {"language", "backend", "benchmark", "mean", "stddev", "iterations"}
    param_cols = [col for col in df.columns if col not in base_cols]

    rust_native_df = (
        df.filter(
            (pl.col("language") == "rust") & (pl.col("backend") == rust_native_backend)
        )
        .select(param_cols + ["mean"])
        .rename({"mean": "rust_mean"})
    )

    if rust_native_df.is_empty():
        return "Could not find native Rust benchmark to compare against."

    other_df = df.filter(
        (pl.col("language") != "rust") | (pl.col("backend") != rust_native_backend)
    )

    if not param_cols:
        # Handle case with no parameters
        if other_df.height > 0 and rust_native_df.height > 0:
            rust_mean = rust_native_df.select("rust_mean").item()
            summary_df = other_df.with_columns(
                (rust_mean / pl.col("mean")).alias("speedup")
            )
        else:
            return "Not enough data for comparison."
    else:
        summary_df = other_df.join(rust_native_df, on=param_cols, how="left")
        summary_df = summary_df.with_columns(
            (pl.col("rust_mean") / pl.col("mean")).alias("speedup")
        )

    summary = []
    # Sort to ensure consistent output order
    sort_cols = ["language", "backend"] + param_cols
    for row in summary_df.sort(sort_cols).iter_rows(named=True):
        implementation = f"{row['language']} ({row['backend']})"

        params_str = ""
        if param_cols:
            params_str = ", ".join(
                f"{p}={row[p]}" for p in param_cols if row.get(p) is not None
            )
            params_str = f" ({params_str})"

        speedup = row.get("speedup")
        if speedup is not None:
            summary.append(
                f"- **{implementation}**{params_str}: `{speedup:.2f}x` speedup vs Rust (native)."
            )
        else:
            summary.append(
                f"- **{implementation}**{params_str}: No corresponding Rust (native) benchmark found."
            )

    return "\n".join(summary)


def compare_benchmarks():
    try:
        print("--- Starting benchmark comparison ---")
        benches_dir = Path(__file__).parent
        results_path = benches_dir / "results.json"
        output_path = benches_dir.parent / "BENCHMARKS.md"

        print(f"Reading results from: {results_path}")
        if not results_path.exists():
            print(f"Error: results.json not found at {results_path}!", file=sys.stderr)
            sys.exit(1)

        with open(results_path) as f:
            data = json.load(f)

        if not data:
            print("Error: results.json is empty!", file=sys.stderr)
            sys.exit(1)

        print("Successfully loaded results.json. Normalizing data with Polars.")
        df = pl.from_dicts(data)
        # Unnest parameter and stats dictionaries
        df = df.unnest("parameters").unnest("stats")

        print("Sorting data.")
        df = df.sort(["benchmark", "language", "backend"])

        print(f"Writing markdown to: {output_path}")
        with open(output_path, "w") as f:
            f.write(
                "This file is automatically generated by the CI. Do not edit manually.\n\n"
            )

            f.write("### Hardware Specifications\n")
            specs = get_system_specs()
            for key, value in specs.items():
                f.write(f"- **{key}**: {value}\n")
            f.write("\n")

            for benchmark_name, group in df.group_by("benchmark"):
                f.write(f"### {str(benchmark_name).capitalize()} Benchmark\n")

                # Select columns to display, dropping any that are all null
                display_cols = [
                    col for col in group.columns if group[col].is_not_null().any()
                ]
                group_display = group.select(display_cols)

                # Create markdown table
                markdown_output = tabulate(
                    group_display, headers="keys", tablefmt="pipe", showindex=False
                )

                if markdown_output:
                    f.write(markdown_output)
                    f.write("\n\n#### Manual Summary\n")
                    summary = generate_manual_summary(group)
                    f.write(summary)
                f.write("\n\n")

        print("--- Finished benchmark comparison successfully ---")

    except Exception as e:
        print(f"An unexpected error occurred: {e}", file=sys.stderr)
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    compare_benchmarks()

