import json
import platform
import sys
from pathlib import Path

import polars as pl
import psutil
from tabulate import tabulate


def get_system_specs():
    specs = {
        "System": platform.system(),
        "Release": platform.release(),
        "Version": platform.version(),
        "Machine": platform.machine(),
        "Processor": platform.processor(),
        "CPU Cores": psutil.cpu_count(logical=False),
        "Logical CPUs": psutil.cpu_count(logical=True),
        "Total Memory": f"{psutil.virtual_memory().total / (1024**3):.2f} GB",
    }
    return specs


def generate_manual_summary(df: pl.DataFrame) -> str:
    rust_native_backend = "simstring-rust (native)"
    rust_native_time_df = df.filter(
        (pl.col("language") == "rust") & (pl.col("backend") == rust_native_backend)
    )

    if rust_native_time_df.is_empty():
        return "Could not find native Rust benchmark to compare against."

    rust_native_time = rust_native_time_df.select("mean").item()

    summary_df = df.filter(
        (pl.col("language") != "rust") | (pl.col("backend") != rust_native_backend)
    ).with_columns(
        (rust_native_time / pl.col("mean")).alias("speedup"),
    )

    summary = []
    for row in summary_df.iter_rows(named=True):
        implementation = f"{row['language']} ({row['backend']})"
        summary.append(
            f"- **{implementation}**: `{row['speedup']:.2f}x` speedup vs Rust (native)."
        )

    return "\n".join(summary)


def compare_benchmarks():
    try:
        print("--- Starting benchmark comparison ---")
        benches_dir = Path(__file__).parent
        results_path = benches_dir / "results.json"
        output_path = benches_dir.parent / "BENCHMARKS.md"

        print(f"Reading results from: {results_path}")
        if not results_path.exists():
            print(f"Error: results.json not found at {results_path}!", file=sys.stderr)
            sys.exit(1)

        with open(results_path) as f:
            data = json.load(f)

        if not data:
            print("Error: results.json is empty!", file=sys.stderr)
            sys.exit(1)

        print("Successfully loaded results.json. Normalizing data with Polars.")
        df = pl.from_dicts(data)
        df = df.unnest("parameters").unnest("stats")

        print("Sorting data.")
        df = df.sort(["benchmark", "language", "backend"])

        print(f"Writing markdown to: {output_path}")
        with open(output_path, "w") as f:
            f.write(
                "This file is automatically generated by the CI. Do not edit manually.\n\n"
            )

            f.write("### Hardware Specifications\n")
            specs = get_system_specs()
            for key, value in specs.items():
                f.write(f"- **{key}**: {value}\n")
            f.write("\n")

            for benchmark_name, group in df.group_by("benchmark"):
                f.write(f"### {str(benchmark_name).capitalize()} Benchmark\n")

                # Select and drop columns that are all null
                group_display = group.select(
                    [col for col in group.columns if group[col].is_not_null().any()]
                )

                # Create markdown table
                markdown_output = tabulate(
                    group_display, headers="keys", tablefmt="pipe", showindex=False
                )

                if markdown_output:
                    f.write(markdown_output)
                    f.write("\n\n#### Manual Summary\n")
                    summary = generate_manual_summary(
                        group.select(["language", "backend", "mean"])
                    )
                    f.write(summary)
                f.write("\n\n")

        print("--- Finished benchmark comparison successfully ---")

    except Exception as e:
        print(f"An unexpected error occurred: {e}", file=sys.stderr)
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    compare_benchmarks()
